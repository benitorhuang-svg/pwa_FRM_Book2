{
  "id": "b2_ch9",
  "title": "第9章：信用風險",
  "number": 9,
  "content": {
    "intro": {
      "title": "第 9 章：信用風險 - 重點詳解 (Detail)",
      "roadmap": {
        "guide": "信用風險是指交易對手無法履行合約義務而造成損失的風險。本章從基礎的信用度量出發，重點介紹了金融機構如何利用行為數據建構評分卡模型，並透過傳統的 Z-score 模型進行企業財務健康診斷。",
        "objectives": "*   理解信用風險的核心要素：違約機率 (PD)、違約損失率 (LGD) 與違約風險曝露 (EAD)。\n*   掌握證據權重 (WOE) 轉換與資訊價值 (IV) 在特徵篩選中的應用。\n*   學會利用回歸模型將違約機率轉換為標準信用分數。",
        "topics": "*   9.1 信用風險的定義與分類\n*   9.2 信用風險度量指標 (PD, LGD, EAD)\n*   9.3 信用風險資料分析與前處理\n*   9.4 信用風險評分卡模型 (Scorecard Model)\n*   9.5 信用評級與轉移矩陣\n*   9.6 自展法 (Bootstrapping) 求生存率\n*   9.7 奧特曼 Z 分模型 (Altman Z-score)"
      },
      "value": {
        "practical": "*   **實務場景**：在零售銀行業務中，根據申請人的收入、年齡、歷史信用紀錄自動產出信用評分。\n*   **考試重點**：掌握 Altman Z-score 的公式組成及其在不同產業下的臨界值（Cut-off points）。",
        "theory": "評分卡模型將 PD 線性映射為分數：$\\text{Score} = \\text{Offset} + \\text{Factor} \\times \\ln(\\text{odds})$。WOE 轉換有效解決了非線性與離群值問題。Z-score 是預測企業破產的經典多變量模型。",
        "further_reading": "*   Altman (1968) 原始論文。"
      },
      "implementation": {
        "python": "*   **分箱與 WOE**：實作單變數單調分箱與 WOE 轉換函數。\n*   **模型訓練**：使用 `LogisticRegression` 訓練模型，並利用 `RandomForestRegressor` 評估特徵重要性。\n*   **分數轉換**：實作 `score_addon()` 將回歸係數轉換為具體的評分卡增量。",
        "logic": "*   信用評分卡：從 PD 到分數的線性映射。\n*   Z-score：綜合多項財務比率的信用加權模型。",
        "scenarios": "| 腳本名稱 | 核心使命 |\n| :--- | :--- |\n| **B2_Ch9_1.py** | **[核心]** 實作信用評分卡模型，涵蓋單變數分箱、WOE 轉換與隨機森林特徵評估。 |\n| **B2_Ch9_2.py** | 演示信用資產數據的基礎統計分析與圖表視覺化。 |\n| **B2_Ch9_3.py** | **[工具]** 實作 Altman Z-score 破產預測模型，計算財務健康指標。 |"
      }
    },
    "body": {
      "9.1": "### 9.1 信用風險的定義與分類：違約的結構化分析\n信用風險代表交易對手因自身經濟能力惡化或意願缺失，無法履行合約規定的還款義務而造成的損失。在現代風控制度中，資深從業人員必須識別信用風險的雙重屬性：**違約風險**（Default Risk）與**信用遷徙風險**（Migration Risk）。\n\n#### 專家決策矩陣：信用風險的層級與特徵\n資深分析師根據風險來源進行決策分類：\n\n| 類別 | 驅動因子 | 風險對象 |\n| :--- | :--- | :--- |\n| **零售信用風險** | 個人收入、還款歷史、就業狀況 | 房貸、車貸、信用卡、小額利貸 |\n| **企業信用風險** | 財務槓桿、獲利能力、產業週期 | 商業貸款、企業債、集團融資 |\n| **主權信用風險** | 國家財政預算、外匯準備金、政局 | 國債、主權擔保融資 |\n| **結算風險** | 交易流程失敗、跨時區不對稱 | 場外交易 (OTC) 結算、外匯互換 |\n\n#### 技術核心：違約的三大核心指標 ($PD, LGD, EAD$)\n信用風險的期望損失 (Expected Loss) 是這三個維度的乘積：\n\n$$\n  EL = PD \\times LGD \\times EAD\n$$\n\n> [!IMPORTANT]\n> 在生產端信用分析中（見 `B2_Ch9_1.py`），資深開發者會使用隨機森林對大量因子進行重要性評分。必須區分「軟數據」（如消費習慣）與「硬數據」（如徵信報告），硬數據通常具備更高的 IV (Information Value) 指標，應給予更高權重。\n\n#### 9.1 資深從業人員行動清單 (Action Items)\n執行信用風險分類前，必須確認：\n- **違約定義對齊**：明確定義「逾期 90 天」或「破產申請」為違約觸發點，確保數據標註一致性。\n- **因子顯著性審核**：利用 IV 與單變量分析 (Univariate Analysis)，剔除與違約相關性極低的噪音特徵。\n- **傳導路徑稽核**：檢查是否存在潛在的結構性風險，如同一母公司下多個子公司的關聯信用鏈。",
      "9.2": "### 9.2 信用風險度量指標：$PD, LGD, EAD$ 的精算映射\n掌握信用風險的核心在於對期望損失的精準拆解。資深從業人員將 $PD$（違約機率）、$LGD$（違約損失率）與 $EAD$（違約風險曝露）視為模型建構的三大支柱。這不僅是內部限額管理的依據，更是巴塞爾協議中計算風險加權資產 (RWA) 的法律前提。\n\n#### 技術核心：精算定義\n- **PD (Probability of Default)**：在特定時間（通常 1 年）內，債務人發生違約的可能性。由行為分數 (B-Score) 或申請分數 (A-Score) 推導。\n- **LGD (Loss Given Default)**：一旦違約，不能回收的資金比例。$LGD = 1 - \\text{回收率}$。\n- **EAD (Exposure at Default)**：違約發生那一刻的未償還餘額總計。\n\n#### 專家決策矩陣：期望損失 vs. 非期望損失\n資深分析師必須識別資本覆蓋的對象：\n\n| 類型 | 描述 | 應對策略 |\n| :--- | :--- | :--- |\n| **期望損失 (EL)** | $PD \\times LGD \\times EAD$ | **透過定價覆蓋**：直接計入資產減值準備與利差 |\n| **非期望損失 (UL)** | EL 之外的隨機波動，統計分佈的標準差 | **透過資本金覆蓋**：由經濟資本 (Economic Capital) 承擔 |\n| **壓力損失 (Stress Loss)** | 極端衰退環境下的損失 | 通過壓力測試與緊急預案管理 |\n\n> [!IMPORTANT]\n> 在數據預處理階段（見 `B2_Ch9_1.py`），若 EAD 數據存在缺失，資深開發者會使用 `RandomForestRegressor` 進行填充，而非簡單地使用中位數。這是因為 EAD 與客戶的累計信用限額及消費習慣存在顯著的相關性。\n\n#### 9.2 資深從業人員行動清單 (Action Items)\n執行度量指標稽核時，必須落實：\n- **回收率動態監控**：LGD 受市場抵押品價值影響劇烈，必須定期更新抵押品的清算價值估計。\n- **循環信用修正**：針對信用卡等產品，EAD 必須包含未來可能的提款額度估計偏移 (LEQ)。\n- **數據窗口校準**：PD 計算必須涵蓋一個完整的經濟週期（Through-the-Cycle），防止在繁榮期低估違約率。",
      "9.3": "### 9.3 信用風險資料分析與前處理：特徵工程的黃金準則\n信用數據通常具備高維、稀疏、且充滿離群值的特徵。資深從業人員將 70% 的建模精力投入到資料前處理中。透過**最佳分箱 (Optimal Binning)** 與 **WOE (Weight of Evidence)** 轉換，可以將雜亂的原始特徵轉化為具備單調解釋力且穩健的統計變量。\n\n#### 技術核心：最佳分箱與 WOE 轉換\n- **最佳分箱**：將連續變數離散化。資深從業人員追求的是「單調性」，即分箱後的違約率應隨變數值穩定上升或下降。\n- **WOE 定義**：衡量某分箱對違約標籤的解釋強度。\n\n$$\n  WOE_i = \\ln\\left( \\frac{\\%\\text{non\\_event}_i}{\\%\\text{event}_i} \\right)\n$$\n\n#### 專家決策矩陣：特徵篩選的 IV 標準\n使用資訊價值 (Information Value) 衡量特徵的預測強度：\n\n| IV 數值 | 預測強度評價 | 決策行動 |\n| :--- | :--- | :--- |\n| **$< 0.02$** | 幾乎無預測能力 | **剔除**：保留僅會增加模型噪音 |\n| **$0.02 \\sim 0.1$** | 較弱預測力 | 考慮作為組合特徵的一部分 |\n| **$0.1 \\sim 0.3$** | 中等預測力 | **優選**：模型的核心候選變數 |\n| **$> 0.5$** | 強力預測（或存在數據洩漏） | **警惕**：需檢查該特徵是否包含未來資訊 |\n\n> [!IMPORTANT]\n> 在 Python 實作中（見 `B2_Ch9_1.py`），資深開發者會使用自定義函數執行單調分箱。若自動分箱無法保證單調，必須手動合併分箱。失去單調性的特徵在金融合規審計（解釋性要求）中極難獲批。\n\n#### 9.3 資深從業人員行動清單 (Action Items)\n執行資料前處理時，必須確認：\n- **空值處理邏輯**：嚴禁盲目刪除缺失值，應檢查「缺失」本身是否包含信用信號（如不填寫收入的客戶通常風險更高）。\n- **異常值蓋帽處理**：對極端連續值進行 99% 分位數縮減，防止其對回歸係數產生不當影響。\n- **共線性排除**：在特徵池中，若兩個變數相關性 $> 0.7$，僅保留 IV 較高的一個。",
      "9.4": "### 9.4 信用風險評分卡模型 (Scorecard Model)：機率到分數的工業轉換\n評分卡模型（Scorecard）是將邏輯回歸產出的違約機率 $p$ 線性映射為直觀分數的標準技術。它之所以被廣泛採用的原因是具備極高的可解釋性：每一分錢、每一個特徵的貢獻分佈都清晰可見，這對於金融監管與客戶解釋至關重要。\n\n#### 技術核心：分數轉換公式\n資深從業人員定義模型的分數偏移 (Offset) 與因子 (Factor)，通常遵循「分數加倍 (PDO)」規則：\n\n$$\n  \\text{Score} = A - B \\ln(\\text{odds}), \\quad \\text{where } \\text{odds} = \\frac{p}{1-p}\n$$\n\n其中 $B$ 決定了機率翻倍時分數下降的幅度，而 $A$ 決定了基準分數水平。\n\n#### 專家決策矩陣：評分卡 vs. 深度學習模型\n在工業場景中選擇最合適的路徑：\n\n| 特性 | 標準評分卡 (Logit + WOE) | 深度學習 (Neural Nets) |\n| :--- | :--- | :--- |\n| **可解釋性** | **核心優勢**：每個變數有明確的加分項 | 極低：難以解釋特定拒絕原因 |\n| **合規性** | 巴塞爾協議高度認可 | 需輔助 SHAP 或 LIME 方能過審 |\n| **穩定性** | 強：過濾了噪音且對小樣本友好 | 弱：容易產生過擬合現象 |\n| **性能 ($KS, AUC$)** | 良好 | **卓越**：適合海量數據的非線性挖掘 |\n\n> [!IMPORTANT]\n> 在生產端實作 `score_addon()` 函數時（見 `B2_Ch9_1.py`），資深開發者會將每個分箱的 WOE 乘以回歸係數。這得到的「子項評分」即為業務人員最終看到的實體評分卡。必須檢查各特徵累計分數的總範圍，確保與系統預設的 [300, 850] 分區間一致。\n\n#### 9.4 資深從業人員行動清單 (Action Items)\n執行評分卡建模後，必須落實：\n- **模型穩定性指數 (PSI) 檢測**：比較訓練樣本與目前市場樣本的分數分佈，PSI $> 0.1$ 代表客戶特徵已發生偏移。\n- **KS 檢定評估**：確認模型對好壞客戶的區分度。KS 應處於 $0.3 \\sim 0.5$ 的健康區間。\n- **業務邊界校準**：針對不同產品設立 Cut-off 分數。高風險產品（如無抵押貸）需設定更高的進入門檻。",
      "9.5": "### 9.5 信用評級與轉移矩陣 (Migration Matrix)：動態風險的馬可夫視角\n信用質量隨時間推移並非靜態，而是不斷發生「遷徙」。**轉移矩陣**描述了在給定期間（如 1 年）內，債務人從一個信用評級（如 AAA）變動為另一個評級（如 BBB）的機率。它是計算信用 VaR 及對沖長期信用違約互換 (CDS) 的核心輸入。資深風險官利用轉移矩陣來預測整個信貸組合的「提前惡化」趨勢。\n\n#### 技術核心：一階馬階夫鏈假設\n我們假設明年的評級僅與今年的評級有關，與過去無關。矩陣中的元素 $P_{ij}$ 代表從狀態 $i$ 移至狀態 $j$ 的機率。每一行的機率總和必須嚴格等於 1。\n\n| 期初評級 | AAA | AA | ... | 違約 (D) |\n| :--- | :--- | :--- | :--- | :--- |\n| **AAA** | 90% | 7% | ... | 0.05% |\n| **AA** | 2% | 85% | ... | 0.5% |\n| **CCC** | 0.01% | 1% | ... | 15% |\n\n#### 專家決策矩陣：遷徙機率異常的識別\n資深分析師應警惕以下現象：\n\n| 常態現象 | 異常預警 | 決策行動 |\n| :--- | :--- | :--- |\n| **對角線佔優** | 矩陣對角線外數值激增 | 代表信用環境劇烈動盪或評級模型失效 |\n| **評級漂移性** | 評級更傾向於下移而非上移 | 需要計提更多的資本金以應對信用惡化 |\n| **吸收態 (Absorbing)**| 違約狀態 (D) 為吸收態 | 進入違約後無法自行恢復，需執行資產處置程序 |\n\n> [!IMPORTANT]\n> 在信用組合管理中（見 `B2_Ch9_2.py` 的統計視覺化），資深從業人員必須將外部評級（如 Moody's, S&P）與內部評級矩陣進行對標。若兩者出現顯著分歧，代表內部風控可能存在過度樂觀或模型滯後的盲點。\n\n#### 9.5 資深從業人員行動清單 (Action Items)\n執行轉移矩陣分析前，必須確認：\n- **數據充足性檢驗**：確保低評級桶位（如 CCC）有足夠的樣本數，防止少數極端事件導致遷徙機率異常跳動。\n- **週期性校準 (TTC vs. PIT)**：明確矩陣是反映整個經濟週期的平均水平 (Through-the-Cycle)，還是在反映當前市場壓力 (Point-in-Time)。\n- **違約連動審核**：檢查是否存在行業內的多個主體同時下移，識別系統性的行業信用崩塌風險。",
      "9.6": "### 9.6 自展法 (Bootstrapping) 求生存率：從違約到時間曲線的演化\n在處理具有時間維度的信用風險（如 5 年期債券）時，我們必須估計資產在各個時點的**生存機率 (Survival Probability)**。**自展法 (Bootstrapping)** 是一種強大的解析技術，能從一組不同期限的信用违约互换 (CDS) 點差中，遞迴地提取出市場隱含的違約強度的期限結構。\n\n#### 技術核心：累積生存機率公式\n假設第 $t$ 期的生存機率為 $S_t$，其隨時間以指數衰減描述：\n\n$$\n  S_t = S_{t-1} \\times e^{-\\lambda_t \\Delta t}\n$$\n\n其中 $\\lambda$ 是該時點的邊際違約率。資深計量師透過將 $S_t$ 與市場報價對齊，反推獲取違約曲線。\n\n#### 專家決策矩陣：违约曲線的形態判讀\n曲線的坡度直接反映了市場對債務人未來的信心：\n\n| 曲線形態 | 解讀 | 風險偏好決策 |\n| :--- | :--- | :--- |\n| **向上傾斜** | 預期短期安全，但長期存在結構性疑慮 | 適合進行短期套利，但長期需嚴密對沖 |\n| **陡降 / 倒掛** | **極致危難**：市場預期近期即將發生違約 | 可能觸發信用違約掉期的集中賠付，需準備流動性 |\n| **平整穩定** | 風險溢價穩定 | 適合建構長期限的信用利差投資組合 |\n\n> [!TIP]\n> 在生產端實作生存率演算法時，資深開發者會使用 `scipy.optimize` 進行 root-finding。必須意識到，「風險中性」下的生存率往往比「真實」生存率要低，這是因為市場價格中包含了風險溢價部分。定價時使用市場隱含值，但在風險規劃中則需考慮真實機率。\n\n#### 9.6 資深從業人員行動清單 (Action Items)\n執行自展法計算前，必須落實：\n- **基準利率對齊**：確保折現使用的無風險利率與 CDS 點差報價的計息慣例完全一致。\n- **插值算法選擇**：在有限的報價點之間，選取「Log-Linear」插值，這能保證提取出的違約強度為分段常數，符合物理假設。\n- **回收率假設審計**：自展法對 $LGD$ 假設極度敏感。必須針對不同受償層級（高級、次級）設定正確的回收預期，$40\\%$ 是市場對高級無擔保債的通用假設。",
      "9.7": "### 9.7 奧特曼 Z 分模型 (Altman Z-score)：企業破產預測的黃金標尺\nEdward Altman 於 1968 年提出的 **Z-score** 模型是企業信用分析的經典多變量模型。它透過將五項核心財務比率（涵蓋流動性、盈利能力、財務槓桿與資產周轉率）進行線性加權，產出一個預測企業在兩年內破產機率的綜合分數。資深核貸員（Underwriter）將其視為快速過濾高風險借款人的第一道防線。\n\n#### 技術核心：五大指標構成\n經典的 Z-score 公式定義如下：\n\n$$\n  Z = 1.2 X_1 + 1.4 X_2 + 3.3 X_3 + 0.6 X_4 + 1.0 X_5\n$$\n\n- $X_1$: 營運資本/總資產 (流動性測度)\n- $X_2$: 保留盈餘/總資產 (利潤積累測度)\n- $X_3$: EBIT/總資產 (資產獲利效率)\n- $X_4$: 股市市值/負債總額 (市場信賴度與槓桿)\n- $X_5$: 銷售額/總資產 (資產周轉率)\n\n#### 專家決策矩陣：Z-score 分數段與決策行動\n資深分析師根據數值所在區域執行不同等級的信用限額管理：\n\n| 分數區間 | 診斷區域 | 信用決策 |\n| :--- | :--- | :--- |\n| **$Z > 2.99$** | **安全區 (Safe Zone)** | 可續貸、增加信用額度、維持低利率 |\n| **$1.81 \\sim 2.99$** | **灰色地段 (Gray Zone)** | **警惕**：增加貸後檢查頻率，要求額外抵押品 |\n| **$Z < 1.81$** | **危險區 (Distress Zone)** | **拒絕**：大概率在兩年內發生破產，應執行催收或減持 |\n\n> [!IMPORTANT]\n> 在生產端實作 `Altman_Z_scorec()` 函數時（見 `B2_Ch9_3.py`），資深開發者必須特別注意係數選取。上述 coefficients 是針對上市製造業的。若分析對象為非上市企業或服務業，必須切換至修正後的 $Z'$ 或 $Z''$ 模型，否則會產生嚴重的誤判風險。\n\n#### 9.7 資深從業人員行動清單 (Action Items)\n執行 Z-score 分析時，必須落實：\n- **數據質量審計**：確保所有財務數據均已剔除非經常性損益，以反映企業真實的經營強度。\n- **跨期趨勢對比**：單點 Z 分數意義有限。必須觀察 Z-score 在過去三年的趨勢；持續下降往往比絕對低分更具警示性。\n- **行業基準校準**：針對特定行業（如高科技研發、重資產基建），應將企業 Z 分數與該行業中值進行對比，識別相對風險地位。\n\n#### 核心技術結論\n信用風險管理是「數據實證」與「商業直覺」的融合。從評分卡精準篩選個人客戶，到 Z-score 診斷企業健康，資深從業人員必須精確掌握這些統計工具，在擴大信貸規模與守住資產質量底線之間，建立一道堅實的模型防線。"
    },
    "examples": [
      {
        "id": "ex1",
        "title": "9.1 信用評分卡模型建模全流程",
        "filename": "B2_Ch9_1.py",
        "code": "# B2_Ch9_1.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\n# B2_Ch9_1_A.py \n#import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nimport plotly.io as pio\nimport chart_studio.plotly\nfrom plotly.offline import plot\nimport plotly.graph_objects as go\nfrom  matplotlib.ticker import FuncFormatter\n\ndata = pd.read_csv(r'C:\\FRM Book\\CreditRisk\\cs-training.csv')\ndata = data.iloc[:,1:]\n\ndata.shape\ndata.info()\n\n\n# B2_Ch9_1_B.py \n# make a fancy table for the corresponding Chinese translation of the column names\ntranslation_map = {'SeriousDlqin2yrs':'两年内是否违约',\n                   'RevolvingUtilizationOfUnsecuredLines':'可用额度比值',\n                   'age':'年龄',\n                   'NumberOfTime30-59DaysPastDueNotWorse':'借贷逾期30-59天数目',\n                   'DebtRatio':'负债率',\n                   'MonthlyIncome':'月收入',\n                   'NumberOfOpenCreditLinesAndLoans':'借贷数量',\n                   'NumberOfTimes90DaysLate':'借贷逾期90天数目',\n                   'NumberRealEstateLoansOrLines':'固定资产贷款量',\n                   'NumberOfTime60-89DaysPastDueNotWorse':'借贷逾期60-89天数目',\n                   'NumberOfDependents':'家属人数'}\n\npio.renderers.default = \"browser\"\ndf_transmap = pd.DataFrame.from_dict(translation_map, orient='index').reset_index()\ndf_transmap.columns = ['English', 'Chinese']\npio.renderers.default = \"browser\"\nfig = go.Figure(data=[go.Table(\n                                header = dict(values=list(df_transmap.columns),\n                                            fill_color = 'paleturquoise',\n                                            align='left'),\n                                cells = dict(values=[df_transmap.English, df_transmap.Chinese],\n                                           fill_color='lavender',\n                                           align='left'))\n])\n\nfig.show()\n\n\n\n# B2_Ch9_1_C.py \n# fill NA by random forest\ndata_process = data.iloc[:,[5,0,1,2,3,4,6,7,8,9]]\n# split to known and unknown\nknown = data_process[data_process.MonthlyIncome.notnull()].values\nunknown = data_process[data_process.MonthlyIncome.isnull()].values\n# training set\nX = known[:,1:]\ny = known[:,0]\n# fitting model\nmodel = RandomForestRegressor(random_state=0, n_estimators=200, max_depth=3, n_jobs=-1)\nmodel.fit(X,y)\n# pridict missing data\npred = model.predict(unknown[:,1:]).round(0)\n# fill missing data\ndata.loc[data['MonthlyIncome'].isnull(),'MonthlyIncome'] = pred\n\n\n# B2_Ch9_1_D.py\n# distribution of number of dependents\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x='NumberOfDependents', data = data)\ntotal = float(len(data))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.0f}%'.format(100 * p.get_height()/total),\n            ha=\"center\") \nax.set_title('Number of dependents count')    \nax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n\n\n\n# B2_Ch9_1_E.py \n# fill missing values of NumberOfDependents\nnum_Dependents = pd.Series([0,1,2,3,4]).copy()\nfor i in data['NumberOfDependents'][data['NumberOfDependents'].isnull()].index:\n    data['NumberOfDependents'][i] = num_Dependents.sample(1)\n\n# missing value and duplicate value deletion\ndata=data.dropna()\ndata=data.drop_duplicates()\n\n\n\n# B2_Ch9_1_F.py \n# age analysis\nfig = plt.figure()\nax = plt.subplot()\nax.boxplot(data['age'])\nax.set_xticklabels(['age'])\nax.set_ylabel('age (years old)')\n\n# remove outlier of age\ndata = data[data['age']>18]\ndata = data[data['age']<100]\n\n\n\n# B2_Ch9_1_G.py \n# analysis for RevolvingUtilizationOfUnsecuredLines and DebtRatio\nfig, (ax1, ax2) = plt.subplots(1,2)\nx1 = data['RevolvingUtilizationOfUnsecuredLines'].astype('float') \nx2 = data['DebtRatio'].astype('float')\nax1.boxplot(x1)\nax2.boxplot(x2)\nax1.set_xticklabels(['RevolvingUtilizationOfUnsecuredLines'])\nax2.set_xticklabels(['DebtRatio'])\nax1.set_ylabel('ratio')\n\n# remove outlier of RevolvingUtilizationOfUnsecuredLines and DebtRatio                    \ndata = data[(data['RevolvingUtilizationOfUnsecuredLines']>=0)&(data['RevolvingUtilizationOfUnsecuredLines']<=1)]\ndata = data[(data['DebtRatio']>=0)&(data['DebtRatio']<=1)]\n\n\n\n# B2_Ch9_1_K.py \n# analysis for number of time of default/past due\nfig, (ax1, ax2, ax3) = plt.subplots(1,3)\nx1 = data['NumberOfTime30-59DaysPastDueNotWorse']\nx2 = data['NumberOfTime60-89DaysPastDueNotWorse']\nx3 = data['NumberOfTimes90DaysLate']\nax1.boxplot(x1)\nax2.boxplot(x2)\nax3.boxplot(x3)\nax1.set_xticklabels(['NumberOfTime30-59DaysPastDueNotWorse'])\nax2.set_xticklabels(['NumberOfTime60-89DaysPastDueNotWorse'])\nax3.set_xticklabels(['NumberOfTimes90DaysLate'])\nax1.set_ylabel('Number of times of past due')\n\n# remove outlier of number of times past due                    \ndata = data[data['NumberOfTime30-59DaysPastDueNotWorse']<20]\ndata = data[data['NumberOfTime60-89DaysPastDueNotWorse']<20]\ndata = data[data['NumberOfTimes90DaysLate']<20]\n\n\n\n# B2_Ch9_1_L.py \n# client with good credit: 1; client with bad credit: 0\ndata['SeriousDlqin2yrs'] = 1-data['SeriousDlqin2yrs'] \nclient_group = data['SeriousDlqin2yrs'].groupby(data['SeriousDlqin2yrs']).count()\ngood_client_percentage = client_group[1]/(client_group[0]+client_group[1])\nbad_client_percentage = client_group[0]/(client_group[0]+client_group[1])\nax = client_group.plot(kind='bar')\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height}', (x + width/2, y + height*1.02), ha='center')    \nax.set_ylabel('Client number')\nprint(\"percentage of good clients: \", format(good_client_percentage*100, '.2f'),\"%\")\nprint(\"percentage of bad clients: \", format(bad_client_percentage*100, '.2f'),\"%\")\n\n\n\n# B2_Ch9_1_M.py\n# age distribution\nax = sns.distplot(data['age'])\nax.set(xlabel='Age', ylabel='Distribution', title='Age distribution')\n\n\n\n# B2_Ch9_1_N.py \n# monthly income distribution\nax = sns.distplot(data[data['MonthlyIncome']<30000]['MonthlyIncome'])\nax.set(xlabel='Monthly income', ylabel='Distribution', title='Monthly income distribution')\n\n\n\n# B2_Ch9_1_O.py \n# heatmap: correlation of columns\ncorr = data.corr()\nfig = plt.figure(figsize=(14, 12))\nsns.heatmap(corr,annot = True, cmap=\"YlGnBu\")\nfig.tight_layout()\n\n\n\n# B2_Ch9_1_P.py \nfrom sklearn.model_selection import train_test_split\n\nY = data['SeriousDlqin2yrs']\nX = data.iloc[:,1:]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.8, random_state=0)\ntrain = pd.concat([Y_train,X_train], axis =1)\ntest = pd.concat([Y_test,X_test], axis =1)\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n# save test data to a file\ntest.to_csv('test.csv', index=False)\n\n\n\n# B2_Ch9_1_Q.py \nimport scipy.stats as stats\nimport numpy as np\ndef monotone_optimal_binning(X, Y, n):\n    r = 0\n    total_good = Y.sum()\n    total_bad = Y.count() - total_good\n    while np.abs(r) < 1:\n        d1 = pd.DataFrame({\"X\": X, \"Y\": Y, \"Bucket\": pd.qcut(X, n)})\n        d2 = d1.groupby('Bucket', as_index = True)\n        r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n        n = n - 1\n    d3 = pd.DataFrame(d2.min().X, columns = ['min_' + X.name])\n    d3['min_' + X.name] = d2.min().X\n    d3['max_' + X.name] = d2.max().X\n    d3[Y.name] = d2.sum().Y\n    d3['total'] = d2.count().Y\n    # calculate WOE\n    d3['Goodattribute']=d3[Y.name]/total_good\n    d3['badattribute']=(d3['total']-d3[Y.name])/total_bad\n    d3['woe'] = np.log(d3['Goodattribute']/d3['badattribute'])\n    # calculate IV\n    iv = ((d3['Goodattribute']-d3['badattribute'])*d3['woe']).sum()\n    d4 = (d3.sort_values(by = 'min_' + X.name)).reset_index(drop = True)\n    print (\"=\" * 80)\n    print (d4)\n    cut = []\n    cut.append(float('-inf'))\n    for i in range(1,n+1):\n        qua =X.quantile(i/(n+1))\n        cut.append(round(qua,4))\n    cut.append(float('inf'))\n    woe = list(d4['woe'].round(3))\n    return d4, iv, cut, woe\n\ndfx1, ivx1, cutx1, woex1 = monotone_optimal_binning(data['RevolvingUtilizationOfUnsecuredLines'], data['SeriousDlqin2yrs'], n = 10)\ndfx2, ivx2, cutx2, woex2 = monotone_optimal_binning(data['age'], data['SeriousDlqin2yrs'], n = 10)\ndfx4, ivx4, cutx4, woex4 = monotone_optimal_binning(data['DebtRatio'], data['SeriousDlqin2yrs'], n = 20)\n\n\n\n# B2_Ch9_1_R.py \ndef self_binning(Y, X, cat):\n    good = Y.sum()\n    bad = Y.count()-good\n    d1 = pd.DataFrame({'X':X,'Y':Y,'Bucket':pd.cut(X,cat)})\n    d2 = d1.groupby('Bucket', as_index = True)\n    d3 = pd.DataFrame(d2.X.min(), columns=['min'])\n    d3['min'] = d2.min().X\n    d3['max'] = d2.max().X\n    d3['sum'] = d2.sum().Y\n    d3['total'] = d2.count().Y\n    d3['rate'] = d2.mean().Y\n    d3['woe'] = np.log((d3['rate'] / (1 - d3['rate'])) / (good / bad))\n    d3['goodattribute'] = d3['sum'] / good\n    d3['badattribute'] = (d3['total'] - d3['sum']) / bad\n    iv = ((d3['goodattribute'] - d3['badattribute']) * d3['woe']).sum()\n    d4 = d3.sort_values(by='min')\n    print(\"=\" * 60)\n    print(d4)\n    woe = list(d4['woe'].round(3))\n    return d4, iv,woe\n\npinf = float('inf')\nninf = float('-inf')\ncutx3 = [ninf, 0, 1, 3, 5, pinf]\ncutx5 = [ninf,1000,2000,3000,4000,5000,6000,7500,9500,12000,pinf]\ncutx6 = [ninf, 1, 2, 3, 5, pinf]\ncutx7 = [ninf, 0, 1, 3, 5, pinf]\ncutx8 = [ninf, 0,1,2, 3, pinf]\ncutx9 = [ninf, 0, 1, 3, pinf]\ncutx10 = [ninf, 0, 1, 2, 3, 5, pinf]\ndfx3, ivx3,woex3 = self_binning(data['SeriousDlqin2yrs'],data['NumberOfTime30-59DaysPastDueNotWorse'],cutx3)\ndfx5, ivx5,woex5 = self_binning(data['SeriousDlqin2yrs'],data['MonthlyIncome'],cutx5)\ndfx6, ivx6,woex6 = self_binning(data['SeriousDlqin2yrs'],data['NumberOfOpenCreditLinesAndLoans'],cutx6) \ndfx7, ivx7,woex7 = self_binning(data['SeriousDlqin2yrs'],data['NumberOfTimes90DaysLate'],cutx7)\ndfx8, ivx8,woex8 = self_binning(data['SeriousDlqin2yrs'],data['NumberRealEstateLoansOrLines'],cutx8) \ndfx9, ivx9,woex9 = self_binning(data['SeriousDlqin2yrs'],data['NumberOfTime60-89DaysPastDueNotWorse'],cutx9)\ndfx10, ivx10,woex10 = self_binning(data['SeriousDlqin2yrs'],data['NumberOfDependents'],cutx10)\n\n\n\n# B2_Ch9_1_S.py \nivlist=[ivx1,ivx2,ivx3,ivx4,ivx5,ivx6,ivx7,ivx8,ivx9,ivx10]\nindex=['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(1)\nx = np.arange(len(index))+1\nax.bar(x, ivlist, width=0.4)\nax.set_xticks(x)\nax.set_xticklabels(index, rotation=0, fontsize=12)\nax.set_xlabel('Variable', fontsize=14)\nax.set_ylabel('Information value', fontsize=14)\n\nfor a, b in zip(x, ivlist):\n    plt.text(a, b+0.01, '%.2f'%b, ha='center', va='bottom', fontsize=10)\n\n\n\n# B2_Ch9_1_T.py \n# woe conversion\ndef woe_conversion(series,cut,woe):\n    list=[]\n    i=0\n    while i<len(series):\n        try:\n            value=series[i]\n        except:\n            i += 1\n            continue\n        j=len(cut)-2\n        m=len(cut)-2\n        while j>=0:\n            if value>=cut[j]:\n                j=-1\n            else:\n                j -=1\n                m -= 1\n        list.append(woe[m])\n        i += 1\n    return list\n\ntrain['RevolvingUtilizationOfUnsecuredLines'] = pd.Series(woe_conversion(train['RevolvingUtilizationOfUnsecuredLines'], cutx1, woex1))\ntrain['age'] = pd.Series(woe_conversion(train['age'], cutx2, woex2))\ntrain['NumberOfTime30-59DaysPastDueNotWorse'] = pd.Series(woe_conversion(train['NumberOfTime30-59DaysPastDueNotWorse'], cutx3, woex3))\ntrain['DebtRatio'] = pd.Series(woe_conversion(train['DebtRatio'], cutx4, woex4))\ntrain['MonthlyIncome'] = pd.Series(woe_conversion(train['MonthlyIncome'], cutx5, woex5))\ntrain['NumberOfOpenCreditLinesAndLoans'] = pd.Series(woe_conversion(train['NumberOfOpenCreditLinesAndLoans'], cutx6, woex6))\ntrain['NumberOfTimes90DaysLate'] = pd.Series(woe_conversion(train['NumberOfTimes90DaysLate'], cutx7, woex7))\ntrain['NumberRealEstateLoansOrLines'] = pd.Series(woe_conversion(train['NumberRealEstateLoansOrLines'], cutx8, woex8))\ntrain['NumberOfTime60-89DaysPastDueNotWorse'] = pd.Series(woe_conversion(train['NumberOfTime60-89DaysPastDueNotWorse'], cutx9, woex9))\ntrain['NumberOfDependents'] = pd.Series(woe_conversion(train['NumberOfDependents'], cutx10, woex10))\ntrain.dropna(how = 'any')\ntrain.to_csv('WoeData.csv', index=False)\n\ntest['RevolvingUtilizationOfUnsecuredLines'] = pd.Series(woe_conversion(test['RevolvingUtilizationOfUnsecuredLines'], cutx1, woex1))\ntest['age'] = pd.Series(woe_conversion(test['age'], cutx2, woex2))\ntest['NumberOfTime30-59DaysPastDueNotWorse'] = pd.Series(woe_conversion(test['NumberOfTime30-59DaysPastDueNotWorse'], cutx3, woex3))\ntest['DebtRatio'] = pd.Series(woe_conversion(test['DebtRatio'], cutx4, woex4))\ntest['MonthlyIncome'] = pd.Series(woe_conversion(test['MonthlyIncome'], cutx5, woex5))\ntest['NumberOfOpenCreditLinesAndLoans'] = pd.Series(woe_conversion(test['NumberOfOpenCreditLinesAndLoans'], cutx6, woex6))\ntest['NumberOfTimes90DaysLate'] = pd.Series(woe_conversion(test['NumberOfTimes90DaysLate'], cutx7, woex7))\ntest['NumberRealEstateLoansOrLines'] = pd.Series(woe_conversion(test['NumberRealEstateLoansOrLines'], cutx8, woex8))\ntest['NumberOfTime60-89DaysPastDueNotWorse'] = pd.Series(woe_conversion(test['NumberOfTime60-89DaysPastDueNotWorse'], cutx9, woex9))\ntest['NumberOfDependents'] = pd.Series(woe_conversion(test['NumberOfDependents'], cutx10, woex10))\ntest.dropna(how = 'any')\n\ntrain_X =train.drop(['NumberRealEstateLoansOrLines','NumberOfDependents','NumberOfOpenCreditLinesAndLoans','DebtRatio','MonthlyIncome'],axis=1)\ntest_X =test.drop(['NumberRealEstateLoansOrLines','NumberOfDependents','NumberOfOpenCreditLinesAndLoans','DebtRatio','MonthlyIncome'],axis=1)\n\n\n\n# B2_Ch9_1_U.py \nfrom sklearn.metrics import roc_curve, auc\nimport statsmodels.api as sm\n\nX_train =train_X.drop(['SeriousDlqin2yrs'],axis =1)\ny_train =train_X['SeriousDlqin2yrs']\ny_test = test_X['SeriousDlqin2yrs']\nX_test = test_X.drop(['SeriousDlqin2yrs'],axis =1)\nX_train = sm.add_constant(X_train)\nmodel = sm.Logit(y_train,X_train)\nresult = model.fit()\nprint(result.summary2())\n\n\n\n# B2_Ch9_1_V.py \nfrom sklearn.metrics import roc_curve, auc\n\nX2 = sm.add_constant(X_test)\nresu = result.predict(X2)\nFPR,TPR,threshold = roc_curve(y_test,resu)\nROC_AUC = auc(FPR,TPR)\nplt.plot(FPR, TPR, 'b', label='AUC = %0.2f' % ROC_AUC)\nplt.legend(loc='lower right')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('TPR')\nplt.xlabel('FPR')\n\n\n\n# B2_Ch9_1_W.py \n# regression coeficients\ncoe = result.params\n\nimport numpy as np\n# set benchmark score as 600; PDO as 10; Odds as 10\nbenchmark = 600\npdo = 10\nodds = 10\nfactor = pdo/np.log(2)\noffset = benchmark-factor*np.log(odds)\nbaseScore = round(offset + factor * coe[0], 0)\n\n\n\n# B2_Ch9_1_X.py \n# function to calculate addon score for a single variable\ndef score_addon(coe,woe,factor):\n    addon = []\n    for w in woe:\n        score = round(coe*w*factor,0)\n        addon.append(score)\n    return addon\n\n# calculate addon score \nx1 = score_addon(coe[1], woex1, factor)\nx2 = score_addon(coe[2], woex2, factor)\nx3 = score_addon(coe[3], woex3, factor)\nx7 = score_addon(coe[4], woex7, factor)\nx9 = score_addon(coe[5], woex9, factor)\nprint('x1: ', x1)\nprint('x2: ', x2)\nprint('x3: ', x3)\nprint('x7: ', x7)\nprint('x9: ', x9)\n\n\n# B2_Ch9_1_Y.py \n# compute score for single variable\ndef single_variable_score(series,cut,score):\n    list = []\n    i = 0\n    while i < len(series):\n        value = series[i]\n        j = len(cut) - 2\n        m = len(cut) - 2\n        while j >= 0:\n            if value >= cut[j]:\n                j = -1\n            else:\n                j -= 1\n                m -= 1\n        list.append(score[m])\n        i += 1\n    return list\n\n\n\n# B2_Ch9_1_Z.py \nfrom pandas import Series\n\ntest = pd.read_csv('test.csv')\ntest['BaseScore']=Series(np.zeros(len(test))) + baseScore\ntest['x1'] = Series(single_variable_score(test['RevolvingUtilizationOfUnsecuredLines'], cutx1, x1))\ntest['x2'] = Series(single_variable_score(test['age'], cutx2, x2))\ntest['x3'] = Series(single_variable_score(test['NumberOfTime30-59DaysPastDueNotWorse'], cutx3, x3))\ntest['x7'] = Series(single_variable_score(test['NumberOfTimes90DaysLate'], cutx7, x7))\ntest['x9'] = Series(single_variable_score(test['NumberOfTime60-89DaysPastDueNotWorse'], cutx9, x9))\ntest['Score'] = test['x1'] + test['x2'] + test['x3'] + test['x7'] +test['x9']  + baseScore\ntest.to_csv('ScoreData.csv', index=False)\n\nfiltered_columns = ['SeriousDlqin2yrs','BaseScore', 'x1', 'x2', 'x3', 'x7', 'x9', 'Score']\ndisplaytable = test.reindex(columns = filtered_columns)\ndisplaytable.head()\n\n\n\n"
      },
      {
        "id": "ex2",
        "title": "9.2 信用資產數據基礎統計分析",
        "filename": "B2_Ch9_2.py",
        "code": "# B2_Ch9_2.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nCDS_spreads = pd.read_csv(\"C:\\\\Dropbox\\\\FRM Book\\\\CreditRisk\\\\CDS_spreads.csv\")\ndf = pd.DataFrame(CDS_spreads)\n\ndf['Survival'] = 0.0\nnumerator1 = 0.0\nnumerator2 = 0.0\ndenominator1 = 0.0\ndenominator2 = 0.0\nterm_final = 0.0\n\ndf['Spread'] = df['Spread']/10000\n\nRR = df.at[0,'Recovery']\nL = 1.0 - RR\n\nt1 = df.at[0, 'Maturity']\nt2 = df.at[1, 'Maturity']\ndelta_t = t2-t1\n\nfor row_index,row in df.iterrows():\n    if(row_index == 0):\n        df.at[0,'Survival'] = 1\n    if(row_index==1):\n        df.at[1,'Survival'] = L / (L + (delta_t * df.at[1,'Spread']))\n    if(row_index>1):\n        temp_counter = row_index\n        term1 = 0.0\n        term2 = 0.0\n        j = 1\n        while(j > row_index-1):\n            numerator1_temp = df.at[row_index,'DF'] * ((L * df.at[row_index - 1,'Survival']) - ((L + (delta_t * df.at[row_index,'Spread']))*(df.at[row_index,'Survival'])))\n            numerator1 = numerator1 + numerator1_temp\n            row_index = row_index - 1\n        row_index = temp_counter\n        denominator1 = ((df.at[row_index,'DF']) * (L + (delta_t * df.at[row_index,'Spread'])))\n        term1_temp = numerator1/denominator1\n        term1 = term1 + term1_temp\n\n        numerator2 = (L * df.at[row_index - 1,'Survival'])\n        denominator2 = (L + (delta_t * df.at[row_index,'Spread']))\n        term2_temp = numerator2/denominator2\n        term2 = term2 + term2_temp\n        term_final = term1 + term2\n        df.at[row_index, 'Survival'] = term_final\n            \nplt.plot(df['Maturity'], df['Survival'])\nplt.title('Survival probability')\nplt.xlabel('Maturity')\nplt.ylabel('Survival probability')\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().xaxis.set_ticks_position('bottom')  \n"
      },
      {
        "id": "ex3",
        "title": "9.3 Altman Z-score 破產預測模型",
        "filename": "B2_Ch9_3.py",
        "code": "# B2_Ch9_3.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\n# calculate Altman Z-Score fore public corporation\ndef Altman_Z_scorec(WC,TA,RE,EBIT,MVE,TL,S):\n    # WC: Working Capital\n    # TA: Total Assests\n    # RE: Retained Earnings\n    # EBIT: Earnings Before Interest and Tax\n    # MVE: Market Value of Equity\n    # TL: Total Liabilities\n    # S: Net Sales\n    \n    X1 = WC/TA;\n    X2 = RE/TA;\n    X3 = EBIT/TA;\n    X4 = MVE/TL;\n    X5 = S/TA;\n     \n    # calculate z-score\n    Z_score = 1.2*X1 + 1.4*X2 + 3.3*X3 + .6*X4 + X5;\n    print('Altman value is ', round(Z_score, 2))\n    \n    # display results\n    if Z_score > 3.0:    \n        print('Business is healthy.')\n    elif Z_score < 1.8:\n        print('Business is bankrupt.')\n    else: \n        print('Business is intermediate.')\n"
      }
    ]
  }
}