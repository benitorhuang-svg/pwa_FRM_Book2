{
  "id": "b2_ch4",
  "title": "第4章：回歸分析",
  "number": 4,
  "content": {
    "intro": {
      "title": "第 4 章：回歸分析 - 重點詳解 (Detail)",
      "roadmap": {
        "guide": "本章介紹各種回歸分析方法在金融建模中的應用。回歸分析是統計學中的核心工具，用於建立變數間的關係模型。在金融領域，回歸分析廣泛應用於風險因子分析、資產定價模型和信用風險評估等方面。",
        "objectives": "*   掌握各種回歸分析方法在金融中的應用。\n*   理解模型選擇與評估的統計方法（如 MSE, $R^2$）。\n*   熟悉正則化技術（Ridge, Lasso）在金融建模中的運用，解決多重共線性問題。",
        "topics": "*   4.1 回歸分析概述\n*   4.2 回歸模型的建模與評估\n*   4.3 線性回歸 (Linear Regression)\n*   4.4 邏輯回歸 (Logistic Regression)\n*   4.5 多項式回歸 (Polynomial Regression)\n*   4.6 嶺回歸 (Ridge Regression)\n*   4.7 套索回歸 (Lasso Regression)"
      },
      "value": {
        "practical": "*   **實務場景**：利用邏輯回歸建立簡單的信用評分模型，預測違約機率。\n*   **考試重點**：理解 Lasso 回歸如何進行特徵選擇（Feature Selection）以及過度擬合（Overfitting）的防治。",
        "theory": "線性回歸旨在最小化殘差平方和 (OLS)。在金融因子模型中，因子數量可能過多且高度相關，正則化（Ridge/Lasso）能有效處理多重共線性，提升泛化能力。",
        "further_reading": "*   Hastie et al., The Elements of Statistical Learning."
      },
      "implementation": {
        "python": "*   **模型建立**：使用 `sklearn.linear_model` 實作線性、邏輯、嶺與套索回歸。\n*   **視覺化**：實作三維線性回歸視覺化，分析多因子對目標變數的聯合影響。\n*   **模型診斷**：使用 `Statsmodels` 進行參數顯著性檢定與 R2 評估。",
        "logic": "*   L2 懲罰 (Ridge)：係數平方和。\n*   L1 懲罰 (Lasso)：係數絕對值之和，實現特徵選擇。",
        "scenarios": "| 腳本名稱 | 核心使命 |\n| :--- | :--- |\n| **B2_Ch4_1.py** | 實作多種函數擬合，演示模型複雜度與預測性能的平衡。 |\n| **B2_Ch4_2.py** | 演示基礎回歸模型建立與圖表視覺化。 |\n| **B2_Ch4_3.py** | 利用 SciPy 與 Statsmodels 進行線性回歸統計分析。 |\n| **B2_Ch4_4.py** | 實作三維線性回歸視覺化，分析多個因子對目標變數的影響。 |\n| **B2_Ch4_5.py** | 演示指數量化模型的基礎回歸與視覺化。 |\n| **B2_Ch4_6.py** | 實作邏輯回歸 (Logistic Regression)，應用於分類問題與混淆矩陣分析。 |\n| **B2_Ch4_7.py** | 實作多項式回歸與 MSE 誤差評估。 |\n| **B2_Ch4_8.py** | 利用統計方法進行線性擬合與參數顯著性檢定。 |\n| **B2_Ch4_9.py** | 實作嶺回歸 (Ridge Regression)，分析正則化參數對係數的擠壓效果。 |\n| **B2_Ch4_10.py** | 實作套索回歸 (Lasso Regression)，展示其特徵選擇特性。 |"
      },
      "body": {
        "4.1": "### 4.1 回歸分析概述：金融變數間的聯動編碼\n回歸分析是量化金融的統計心臟，用於建立目標變數（標的價格、風險因子）與解釋變數之間的數學映射。資深從業人員必須將回歸視為一種「信息提取」過程，旨在從市場噪音中分離出系統性的結構特徵。\n\n#### 專家決策矩陣：模型複雜度與泛化能力\n在金融建模中，選擇合適的模型複雜度是預測成敗的核心決策：\n\n| 特度級別 | 性質 | 風險特徵 | 解決方案 |\n| :--- | :--- | :--- | :--- |\n| **欠擬合 (Underfitting)** | 模型過於簡單，無法捕捉趨勢 | 高偏誤 (High Bias) | 增加多項式階數或引入新因子 |\n| **最優擬合 (Optimal)** | 均衡捕捉信號與噪音 | **最低概括誤差** | 交叉驗證 (Cross-Validation) |\n| **過擬合 (Overfitting)** | 完美配適歷史，捕捉了隨機噪音 | 高方差 (High Variance) | 引入正則化 (Ridge/Lasso) |\n\n#### 技術核心：偏誤-方差權衡 (Bias-Variance Trade-off)\n總預測誤差可分解為偏誤、方差與不可解噪音。資深分析師的目標是極小化總誤差，而非單純追求 $R^2$：\n\n$$\n  \\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\sigma_{noise}^2\n$$\n\n> [!IMPORTANT]\n> 在 **Scikit-learn** 的 `PolynomialFeatures` 實作中（見 `B2_Ch4_1.py`），高階多項式（如 $d > 5$）在獲取更高擬合度的同時，往往會導致模型的泛化性能崩潰。資深開發者必須通過交叉驗證曲線來確定模型複雜度的「甜蜜點」。\n\n#### 4.1 資深從業人員行動清單 (Action Items)\n執行回歸建模前，必須確認：\n- **複雜度預算管理**：限制多項式階數，優先使用具備經濟學意義的因子模型。\n- **樣本路徑一致性**：確保訓練集與測試集的數據分佈一致，無結構性斷裂。\n- **誤差分解審計**：監控訓練集與驗證集誤差的差異，識別潛在的過擬合風險。\n\n#### 核心技術結論\n回歸分析不僅是擬合曲線的工具，更是對市場邏輯的實證。我們追求的是能在未知數據下保持穩定的模型，而非歷史數據的完美複製品。",
        "4.2": "### 4.2 回歸模型的建模與評估：精確度的統計判讀\n回歸模型的價值不在於模型本身的複雜度，而在於其對數據解釋的「質量」。資深從業人員必須建立一套嚴謹的評估體系，從殘差分佈、決定係數到泛化誤差，全面審核模型的健康度。\n\n#### 專家定義：核心評估指標\n我們對回歸模型進行多維度審核，其核心數學定義如下：\n- **均方誤差 (MSE)**：衡量預測點與實際點的平均距離。$\\text{MSE} = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$。\n- **決定係數 ($R^2$)**：衡量模型解釋數據變異的比例。$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$。\n- **殘差分析**：殘差必須符合「白噪聲」特徵，即無序列相關、均值為零、方差恆定。\n\n#### 決策矩陣：模型評估指標的選擇\n指標的選擇必須服務於業務目的：\n\n| 指標 | 優點 | 實務警示 |\n| :--- | :--- | :--- |\n| **$R^2$** | 直觀，範圍在 0 到 1 之間 | 增加任何新變量都會推高 $R^2$，容易產生虛假信賴 |\n| **Adjusted $R^2$** | 對變量數量進行懲罰 | 它是資深分析師在多因子建模中的首選指標 |\n| **RMSE** | 與目標變量單位一致，易於解釋利潤損失 | 對極端異常值極其敏感 |\n\n> [!IMPORTANT]\n> 在 **Statsmodels** 實作中（見 `B2_Ch4_3.py`），我們必須特別關注 $p$-value。若變量不具備統計顯著性 ($p > 0.05$)，即便 $R^2$ 很高，該變量在生產環境中也僅是噪音，必須予以剔除。\n\n#### 4.2 資深從業人員行動清單 (Action Items)\n執行模型評估時，必須落實：\n- **顯著性二次元檢查**：確認 F-test 的整體顯著性與個別 t-test 的變量顯著性。\n- **殘差正態性檢定**：利用 Jarque-Bera 檢驗殘差是否具備肥尾或偏斜特徵。\n- **多重共線性監控**：計算 VIF 指標，確保解釋變量之間不存在嚴重的共線性干擾。\n\n#### 核心技術結論\n模型評估是量化建模的「過濾網」。我們透過嚴格的統計紅線，確保進入生產環境的模型具備真實的解釋力與預測能力。",
        "4.3": "### 4.3 線性回歸 (Linear Regression)：金融建模的基本粒子\n線性回歸是所有量化模型的起點。在金融領域，它定義了資產收益與風險因子之間的線性關係。其經典應用包括求解資產的 Beta 值與信用風險的敏感度分析。資深從業人員視線性回歸為一種精簡、解析性強且具備經濟學直覺的強大工具。\n\n#### 專家定義：最小平方法 (OLS)\n線性回歸旨在尋找權重向量 $\\beta$，使得殘差平方和 (RSS) 最小化：\n\n$$\n  \\min_{\\beta} || \\mathbf{y} - \\mathbf{X}\\beta ||^2\n$$\n\n#### 專家決策矩陣：線性回歸的四大金律假設\n若假設被破壞，OLS 將不再是「最佳線性無偏估計量 (BLUE)」：\n\n| 假設項 | 物理意義 | 實務影響 | 解決方案 |\n| :--- | :--- | :--- | :--- |\n| **線性性 (Linearity)** | 特徵與目標呈線性關係 | 關係若為非線性，誤差將激增 | 引入多項式項或變量轉換 |\n| **無自相關 (No Auto-corr)** | 殘差之間相互獨立 | 導致 $p$-value 失真，誇大顯著性 | 使用 Newey-West 標準誤調整 |\n| **同方差性 (Homoscedasticity)** | 誤差項方差恆定 | 參數估計精度下降，區間預測無效 | 使用 WLS 或方差穩健標誤 |\n| **無多重共線性** | 解釋變量不應高度相關 | 矩陣逆運算不穩定，係數數值崩潰 | 使用 Ridge 或 PCA 降維 |\n\n> [!IMPORTANT]\n> 在生產端實施 CAPM 模型或因子模型時（見 `B2_Ch4_3.py`），資深開發者會使用 `stats.linregress` 獲取斜率與截距。若發現截距 $\\alpha$ 顯著異於零，代表存在未被因子解釋的超額回報，這是阿爾法策略的核心來源。\n\n#### 4.3 資深從業人員行動清單 (Action Items)\n執行線性回歸前，必須確認：\n- **數據標準化**：針對多因子模型，確保所有輸入特徵具備一致的量綱（Scaling）。\n- **異常值診斷**：利用 Cook's Distance 識別並移除對回歸路徑有破壞性影響的離群點。\n- **穩健性測試**：觀察模型在不同時間區間內的係數穩定度（Stability Check）。\n\n#### 核心技術結論\n線性回歸是解碼金融數據的第一步。它透過簡潔的斜率與截距，將複雜的市場動態轉化為可量化、可對沖的敏感度參數。",
        "4.4": "### 4.4 邏輯回歸 (Logistic Regression)：違約機率的機率轉換\n在信用風險管理中，目標變量通常是二元的（違約或不違約）。線性回歸在此類場景下會導致預測機率超出 $[0, 1]$ 範圍。**邏輯回歸**透過應用 Sigmoid 函數，將線性輸出轉換為嚴格的機率值，是構建信用評分模型 (Credit Scoring) 的工業基石。\n\n#### 專家定義：對數機率 (Log-Odds) 轉換\n邏輯回歸透過對機率 $p$ 進行對數勝算比轉換，將分類問題重新定義為線性關係：\n\n$$\n  \\ln\\left( \\frac{p}{1-p} \\right) = \\beta_0 + \\sum \\beta_i x_i\n$$\n\n對應的機率預測公式（Sigmoid 函數）如下：\n\n$$\n  p = \\frac{1}{1 + e^{-(\\beta_0 + \\sum \\beta_i x_i)}}\n$$\n\n#### 決策矩陣：邏輯回歸 vs. 決策樹 (XGBoost)\n資深從業人員必須平衡模型的可解釋性與預測性能：\n\n| 特度維度 | 邏輯回歸 (Logistic) | 梯度提升樹 (XGBoost) |\n| :--- | :--- | :--- |\n| **可解釋性** | **極高**：係數直接代表特徵對違約勝算的影響 | 較低：黑盒模型，需依賴 SHAP 值解釋 |\n| **非線性處理** | 弱：需手動構建交叉項 | 強：自動捕捉複雜的非線性特徵交互 |\n| **合規性要求** | **首選**：符合巴塞爾協議對信用透明度的要求 | 需經過額外的可解釋性審查才能上線 |\n| **計算速度** | 極快 | 較慢，需大量算力進行迭代 |\n\n> [!IMPORTANT]\n> 在生產端部署信用評分模型時（見 `B2_Ch4_6.py`），資深開發者會使用 `confusion_matrix` 進行模型診斷。必須特別監控「偽陰性 (False Negative)」，即模型預測不違約但最終違約的情況。在貸前審查中，偽陰性的代價遠高於偽陽性。\n\n#### 4.4 資深從業人員行動清單 (Action Items)\n執行邏輯回歸建模前，必須確認：\n- **數據平衡處理**：若違約樣本過少，需使用 SMOTE 或類別權重調整 (Class Weight) 以避免模型偏見。\n- **特徵工程審計**：對類別型變數進行 One-Hot 編碼，並剔除共線性過高的財務指標。\n- **機率臨界值校準**：不應盲目使用預設的 0.5 作为判定閾值，需根據銀行風險偏好調整選取最優的 Cut-off 值。",
        "4.5": "### 4.5 多項式回歸 (Polynomial Regression)：捕捉市場的非線性共振\n當線性回歸無法解釋複雜的資產價格波動（如權證的非線性 Gamma 效應）時，我們透過引入解釋變數的高階項來進行曲線擬合。**多項式回歸**是處理局部非線性關係的簡便工具，但必須嚴防參數過度膨脹導致的邊界失效。\n\n#### 技術核心：多維度擴張\n我們將原始特徵 $x$ 轉換為一組多項式特徵矩陣，隨後套用標準 OLS 進行求解：\n\n$$\n  y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_d x^d + \\epsilon\n$$\n\n#### 決策矩陣：階數 $d$ 的決策策略\n階數的選取直接決定了模型的生命力：\n\n| 階數層級 | 表現特徵 | 財金實務評價 |\n| :--- | :--- | :--- |\n| **$d=1$** | 標準線性回歸 | 往往存在欠擬合，忽略了市場波動的細微曲率 |\n| **$d=2 \\sim 3$** | 捕捉凸性 (Convexity) | **資深分析師首選**：具備物理意義（如加速度）且較穩健 |\n| **$d > 5$** | 強力擬合噪音 | **禁止**：將出現龍格現象 (Runge's Phenomenon)，邊界預測會極劇跳動 |\n\n> [!IMPORTANT]\n> 在 **Scikit-learn** 的 `Pipeline` 實作中（見 `B2_Ch4_7.py`），必須隨時監控 MSE 的變化。若訓練集 MSE 驟降而測試集 MSE 激增，代表模型已陷入「自嗨模式」（過擬合），此時必須降低階數或引入正則化代償。\n\n#### 4.5 資深從業人員行動清單 (Action Items)\n執行多項式回歸前，必須落實：\n- **階數最優化檢索**：通過 Grid Search 搭配交叉驗證，選取泛化能力最強的階數參數。\n- **數值穩定性校驗**：高階項會導致特徵量綱差異巨大，必須配合 `StandardScaler` 進行數據縮放。\n- **外部數據驗證**：在完成歷史擬合後，使用一組前所未有的「外部驗證集」測試模型的邊界穩定性。",
        "4.6": "### 4.6 嶺回歸 (Ridge Regression)：對抗多重共線性的 L2 懲罰\n在多因子模型中，因子之間的高度相關（多重共線性）會導致 OLS 估計出的係數方差極大，甚至導致模型崩潰。**嶺回歸**透過引入 L2 正則化項（係數平方和），強制縮減不穩定係數的數值，從而大幅提升模型在噪音環境下的穩健性。\n\n#### 專家定義：L2 懲罰函數\n嶺回歸透過懲罰係數的長度，在擬合度與係數規模間取得平衡：\n\n$$\n  \\min_{\\beta} \\left\\{ \\sum (y_i - \\mathbf{x}_i^T\\beta)^2 + \\alpha \\sum \\beta_j^2 \\right\\}\n$$\n\n#### 決策矩陣：懲罰參數 $\\alpha$ 的物理效應\n$\\alpha$ 的大小直接反映了我們對噪音的容忍度：\n\n| $\\alpha$ 級別 | 對係數的影響 | 偏差-方差 表現 |\n| :--- | :--- | :--- |\n| **$\\alpha \\to 0$** | 退化為標準 OLS 回歸 | 低偏差，高方差（易受噪音干擾） |\n| **$\\alpha$ 適中** | 係數向零縮減但不會變為零 | **最佳平衡點**：顯著提升預測精度 |\n| **$\\alpha \\to \\infty$** | 係數全部趨近於零 | 高偏差，低方差（模型失去解釋力） |\n\n> [!IMPORTANT]\n> 在生產端實作中（見 `B2_Ch4_9.py`），資深開發者會使用 `Ridge(normalize=True)` 或手動進行特徵縮放。這至關重要，因為 L2 懲罰對特徵的量綱非常敏感。若未進行正規化，量綱較大的因子將獲得不公平的懲罰優勢。\n\n#### 4.6 資深從業人員行動清單 (Action Items)\n執行 Ridge 回歸前，必須確認：\n- **係數跡線圖審核 (Trace Plots)**：繪製不同 $\\alpha$ 下的係數縮減曲線，觀察哪些因子最先被抑制。\n- **特徵量綱對白**：確保所有輸入因子（如利率、市盈率、回報率）均已轉換為標準正態分佈。\n- **交叉驗證選優**：利用 `RidgeCV` 自動搜索最優的 $\\alpha$ 參數，確保正則化強度具備數據支撐。",
        "4.7": "### 4.7 套索回歸 (Lasso Regression)：L1 正則與自動化特徵選擇\n在金融大數據環境中，面對成百上千個潛在因子，資深分析師需要一個能自動「剔除廢棄物」的模型。**Lasso 回歸**透過 L1 正則化項（係數絕對值之和），具備將無貢獻因子係數直接縮減為 0 的特性，實現了從高維數據到核心因子的精確提煉。\n\n#### 專家定義：L1 懲罰與稀疏性\nLasso 的損失函數引入了絕對值懲罰，這在幾何上會導致最優解出現在座標軸上（即產生零係數）：\n\n$$\n  \\min_{\\beta} \\left\\{ \\sum (y_i - \\mathbf{x}_i^T\\beta)^2 + \\alpha \\sum |\\beta_j| \\right\\}\n$$\n\n#### 決策矩陣：Lasso vs. Ridge 在因子模型中的選擇\n資深計量師需根據因子的結構特性選擇最優正則化工具：\n\n| 特性 | Ridge (L2) | Lasso (L1) |\n| :--- | :--- | :--- |\n| **特徵選擇** | **不支持**：係數僅會縮減但不會歸零 | **支持**：能將無關因子直接踢出模型 |\n| **共線性處理** | 較佳：能穩定處理高度相關的變量組 | 若多個變量相關，Lasso 可能隨機選取其中一個 |\n| **模型解讀** | 包含所有因子，適合解釋整體效應 | **極簡**：僅保留關鍵因子，適合自動化篩選 |\n| **懲罰幾何** | 圓形約束域 (Circle) | 菱形約束域 (Diamond) |\n\n> [!IMPORTANT]\n> 在自動化因子提取系統中（見 `B2_Ch4_10.py`），Lasso 是首選工具。資深開發者會利用其稀疏性來降低生產系統的數據依賴。然而，若預期所有因子皆有微弱貢獻，則 Lasso 可能會因過度剔除而導致模型「貧血」。\n\n#### 4.7 資深從業人員行動清單 (Action Items)\n執行 Lasso 模型前，必須確認：\n- **因子稀疏度審計**：監控係數歸零的比例。若超過 90% 係數歸零，需調低 $\\alpha$ 或重新檢視特徵池質量。\n- **參數靈敏度測試**：測試 $\\alpha$ 的微小波動是否會導致模型保留的因子出現「閃爍」現象（不穩定）。\n- **預測偏差校準**：由於正則化會引入人為偏差，在確定關鍵特徵後，應考慮進行「Post-Lasso OLS」以獲得更準確的係數估算。\n\n#### 核心技術結論\nLasso 是金融建模中的「奧卡姆剃刀」。它在保證模型強度的同時，透過數學手段實踐了極簡主義，是構建跨市場多因子風險矩陣的核心技術。"
      }
    },
    "examples": [
      {
        "id": "ex1",
        "title": "4.1 模型複雜度與性能平衡",
        "filename": "B2_Ch4_1.py",
        "code": "# B2_Ch4_1.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# define functions for data point generation\ndef fun1(x):\n    return -2*x+3\n\ndef fun2(x):\n    return 2*x+1\n\ndef fun3(x):\n    return np.sin(1.5 * np.pi * x)\n\ndef fun4(x):\n    return np.cos(2.1 * np.pi * (x-1.))+np.cos(3 * np.pi * x)\n\n\nnp.random.seed(6)\n\nnum_sample = 30\n\nX = np.sort(np.random.rand(num_sample))\n\nrows = 2\ncols = 2\nfig, axs = plt.subplots(rows, cols, figsize=(14,8))\n\n# fig1\ny1 = fun1(X) + np.random.randn(num_sample) * 0.1\npolynomial_features = PolynomialFeatures(degree=1, include_bias=False)\nlinear_regression = LinearRegression()\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                     (\"linear_regression\", linear_regression)])\npipeline.fit(X[:, np.newaxis], y1)\n\nX_test = np.linspace(0, 1, 1000)\naxs[0, 0].plot(X_test, pipeline.predict(X_test[:, np.newaxis]), color='red', label=\"Fitting model\")\naxs[0, 0].scatter(X, y1)\naxs[0, 0].set_yticks([1.0, 1.5, 2.0, 2.5, 3.0])\naxs[0, 0].set_title('(a)', loc='left')\n\n# fig2\ny2 = fun2(X) + np.random.randn(num_sample) * 0.1\npolynomial_features = PolynomialFeatures(degree=1, include_bias=False)\nlinear_regression = LinearRegression()\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                     (\"linear_regression\", linear_regression)])\npipeline.fit(X[:, np.newaxis], y2)\n\nX_test = np.linspace(0, 1, 1000)\naxs[0, 1].plot(X_test, pipeline.predict(X_test[:, np.newaxis]), color='red', label=\"Fitting model\")\naxs[0, 1].scatter(X, y2)\naxs[0, 1].set_yticks([1.0, 1.5, 2.0, 2.5, 3.0])\naxs[0, 1].set_title('(b)', loc='left')\n\n# fig3\ny3 = fun3(X) + np.random.randn(num_sample) * 0.1\npolynomial_features = PolynomialFeatures(degree=5, include_bias=False)\nlinear_regression = LinearRegression()\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                     (\"linear_regression\", linear_regression)])\npipeline.fit(X[:, np.newaxis], y3)\n\nX_test = np.linspace(0, 1, 1000)\naxs[1, 0].plot(X_test, pipeline.predict(X_test[:, np.newaxis]), color='red', label=\"Fitting model\")\naxs[1, 0].scatter(X, y3)\naxs[1, 0].set_title('(c)', loc='left')\n\n# fig4\ny4 = fun4(X) + np.random.randn(num_sample) * 0.1\npolynomial_features = PolynomialFeatures(degree=8, include_bias=False)\nlinear_regression = LinearRegression()\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                     (\"linear_regression\", linear_regression)])\npipeline.fit(X[:, np.newaxis], y4)\n\nX_test = np.linspace(0, 1, 1000)\naxs[1, 1].plot(X_test, pipeline.predict(X_test[:, np.newaxis]), color='red', label=\"Fitting model\")\naxs[1, 1].scatter(X, y4)\naxs[1, 1].set_yticks([-1.0, 0.0, 1.0, 2.0])\naxs[1, 1].set_title('(d)', loc='left')\n"
      },
      {
        "id": "ex2",
        "title": "4.2 基礎回歸建立與視覺化",
        "filename": "B2_Ch4_2.py",
        "code": "# B2_Ch4_2.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ndef original_fun(X):\n    return np.sin(1.5 * np.pi * X)\n\nnp.random.seed(6)\n\nnum_sample = 30\ndegrees = [1, 5, 15]\ntitles = ['(a) Underfitting', '(b) Optimalfitting', '(c) Overfitting']\nX = np.sort(np.random.rand(num_sample))\ny = original_fun(X) + np.random.randn(num_sample) * 0.1\n\nrows = 1\ncols = 3\nfig, axs = plt.subplots(rows, cols, figsize=(14,5))\n\nfor i in range(len(degrees)):\n    polynomial_features = PolynomialFeatures(degree=degrees[i],\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    X_test = np.linspace(0, 1, 100)\n    axs[i].plot(X_test, pipeline.predict(X_test[:, np.newaxis]), color='red', label=\"Fitting model\")\n    axs[i].plot(X_test, original_fun(X_test), color='lightblue', label=\"Original function\")\n    axs[i].scatter(X, y, s=20, label=\"Samples\")\n    axs[i].set_xlim(0, 1)\n    axs[i].set_ylim(-2, 2)\n    axs[i].set_xticks([0.0, 0.5, 1.0])\n    axs[i].set_yticks([-2, -1, 0, 1, 2])\n    axs[i].legend(loc=\"best\")\n    axs[i].set_title(titles[i], loc='left')\n"
      },
      {
        "id": "ex3",
        "title": "4.3 線性回歸統計分析",
        "filename": "B2_Ch4_3.py",
        "code": "# B2_Ch4_3.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport pandas as pd\nimport datetime as dt\n\n# WTI price        \ndf_WTIPrice = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\WTI.csv', sep=',', usecols=['Date', 'Price'])\n# hazard rate of an energy company\ndf_HazardRate = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\HazardRate.csv', sep=',', usecols=['Date', 'HazardRate'])\n# merge harzard rate file and wti price file\ndf_dwr = df_HazardRate.merge(df_WTIPrice, left_on='Date', right_on='Date', how = 'inner')\n\ndf_dwr['Date'] = pd.to_datetime(df_dwr['Date'])\ndf_dwr = df_dwr[(df_dwr['Date']>=dt.datetime(2008, 8, 30))&(df_dwr['Date']<=dt.datetime(2008, 10, 30))]\nxdata = df_dwr['Price']\nydata = df_dwr['HazardRate']\nplt.plot(xdata, ydata, 'o', label='data')\n\n# linear regression between hazard rate of an energy company and wti\nslope, intercept, r_value, p_value, std_err = stats.linregress(xdata,ydata)\nprint('slope: %f, intercept: %f, r_value: %f, p_value: %f, std_err: %f' % (slope, intercept, r_value, p_value, std_err))\n\nR_squared = r_value*r_value\nprint('R squared: %.2f' % R_squared)\nrline = intercept + slope*xdata\n\nplt.plot(xdata, rline,'r-')\nplt.title('Simple Linear Regression')\nplt.xlabel('WTI')\nplt.ylabel('Hazard Rate')\nplt.gca().set_yticks([0.005, 0.010, 0.015, 0.020])\nplt.legend(['Observed Data', 'y=%5.4f+%5.5f×x, R²=%5.2f' % (intercept, slope, r_value**2)])\n\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().xaxis.set_ticks_position('bottom')\n"
      },
      {
        "id": "ex4",
        "title": "4.4 三維線性回歸視覺化",
        "filename": "B2_Ch4_4.py",
        "code": "# B2_Ch4_4.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\n# B2_Ch4_4_A.py\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n# B2_Ch4_4_B.py\n# read data\ndf = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\MultiLrRegrData.csv')\ndf.head()\n\n\n# B2_Ch4_4_C.py\n# plot stock index price vs interest rate and unemployment rate\nmpl.style.use('ggplot')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\nax1.scatter(df['InterestRate'], df['StockIndexPrice'], color='red')\nax1.set_title('(a) Stock index price VS interest rate', loc='left', fontsize=14)\nax1.set_xlabel('Interest rate', fontsize=14)\nax1.set_ylabel('Stock index price', fontsize=14)\nax1.set_yticks([700, 900, 1100, 1300, 1500])\nax1.grid(True)\n\nax2.scatter(df['UnemploymentRate'], df['StockIndexPrice'], color='green')\nax2.set_title('(b) Stock index price VS unemployment rate', loc='left', fontsize=14)\nax2.set_xlabel('Unemployment rate', fontsize=14)\nax2.set_ylabel('Stock index price', fontsize=14)\nax2.grid(True)\n\n\n# B2_Ch4_4_D.py\n# implement linear regression model\nx = df[['InterestRate','UnemploymentRate']] \ny = df['StockIndexPrice']\nMultiLrModel = LinearRegression()\nMultiLrModel.fit(x, y)\n\n# plot multiple regression model\nfig = plt.figure()\nax = plt.axes(projection='3d')\nzdata = df['StockIndexPrice']\nxdata = df['InterestRate']\nydata = df['UnemploymentRate']\nax.scatter(xdata, ydata, zdata, c=zdata)\nx3d, y3d = np.meshgrid(xdata, ydata)\nz3d_pred = MultiLrModel.intercept_+MultiLrModel.coef_[0]*x3d+MultiLrModel.coef_[1]*y3d\nax.plot_surface(x3d, y3d, z3d_pred, color = 'grey', rstride = 100, cstride = 100, alpha=0.3)\nax.set_title('Multiple Linear Regression', fontsize=14)\nax.set_xlabel('Interest rate')\nax.set_ylabel('Unemployment rate')\nax.set_zlabel('Stock index price')\n\n\n# B2_Ch4_4_E.py\nzdata_pred = MultiLrModel.intercept_+MultiLrModel.coef_[0]*xdata+MultiLrModel.coef_[1]*ydata\nrmse = (np.sqrt(mean_squared_error(zdata, zdata_pred)))\nr2 = r2_score(zdata, zdata_pred)\nprint('RMSE of this polynomial regression model: %.2f' % rmse)\nprint('R square of this polynomial regression model: %.2f' % r2)\n"
      },
      {
        "id": "ex5",
        "title": "4.5 指數量化模型預測分析",
        "filename": "B2_Ch4_5.py",
        "code": "# B2_Ch4_5.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\nimport matplotlib.pylab as plt\nimport numpy as np\n\nx = np.arange(-8, 8, 0.1)\nsig = 1 / (1 + np.exp(-x))\nplt.plot(x, sig)\nplt.title('Sigmoid Function')\nplt.xlabel('x')\nplt.ylabel('p')\nplt.show()\n"
      },
      {
        "id": "ex6",
        "title": "4.6 邏輯回歸二元分類分析",
        "filename": "B2_Ch4_6.py",
        "code": "# B2_Ch4_6.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\n# B2_Ch4_6_A.py\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n\n# B2_Ch4_6_B.py\nbankdata = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\BankTeleCompaign.csv')\nbankdata = bankdata.dropna()\nbankdata.head()\n\n# B2_Ch4_6_C.py\n# plot related item/column \nsns.set(palette=\"pastel\")\nfig, ax = plt.subplots(3, 2, figsize=(6, 8))\nsns.countplot(y=\"job\",  data=bankdata, ax=ax[0, 0])\nsns.countplot(x=\"marital\", data=bankdata, ax=ax[0, 1])\nsns.countplot(x=\"default\", data=bankdata, ax=ax[1, 0])\nsns.countplot(x=\"housing\", data=bankdata, ax=ax[1, 1])\nsns.countplot(x=\"loan\", data=bankdata, ax=ax[2, 0])\nsns.countplot(x=\"poutcome\", data=bankdata, ax=ax[2, 1])\nplt.tight_layout()\n\n# B2_Ch4_6_D.py\n# create dunny variables with only two values: 0 or 1\ndata = pd.get_dummies(bankdata, columns =['job', 'marital', 'default', 'housing', 'loan', 'poutcome'])\n# drop unknow columns\ndata.drop([col for col in data.columns if 'unknow' in col], axis=1, inplace=True)\n# plot correlation heatmap\nsns.heatmap(data.corr(), square=True, cmap=\"YlGnBu\", linewidths=.01, linecolor='lightgrey', cbar_kws={\"orientation\": \"horizontal\", \"shrink\": 0.3, \"pad\": 0.25})\n\n# B2_Ch4_6_E.py\n# split data into training and test sets\nX = data.iloc[:,1:]\ny = data.iloc[:,0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# implement logistic regression model\nmodelclassifier = LogisticRegression(random_state=0)\nmodelclassifier.fit(X_train, y_train)\n\n\n# B2_Ch4_6_F.py\n# evaluate model via confusion matrix \n# evaluate performance of classification model on a set of test dataset with known true values \ny_pred = modelclassifier.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)\n\n\n# B2_Ch4_6_G.py\n# evaluate model by accuracy\nmodel_score = modelclassifier.score(X_test, y_test)\nprint('Model accuracy on test set: {:.2f}'.format(model_score))\n"
      },
      {
        "id": "ex7",
        "title": "4.7 多項式回歸與誤差評估",
        "filename": "B2_Ch4_7.py",
        "code": "# B2_Ch4_7.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\n# B2_Ch4_7_A.py\n# importing libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib as mpl\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# B2_Ch4_6_B.py\ndata = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\PolyRegrData.csv')\n\n# plot data\nmpl.style.use('ggplot')\nplt.figure(figsize=(14,8))\nplt.scatter(data.iloc[:,0].values,data.iloc[:,1].values, c='#1f77b4')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Raw Data')\n\n\n# B2_Ch4_6_C.py\n# preprocess input data\nx = data.iloc[:,0].values.reshape(-1, 1)\ny = data.iloc[:,1].values.reshape(-1, 1)\npolynomial_features= PolynomialFeatures(degree=3)\nx_poly = polynomial_features.fit_transform(x)\n# create and then fit model\nLRmodel = LinearRegression()\nLRmodel.fit(x_poly,y)\nprint('intercept:', LRmodel.intercept_)\nprint('slope:', LRmodel.coef_)\n\n# plot\nplt.plot(x,y,'o',c='#1f77b4')\ny_poly_pred = LRmodel.predict(x_poly)\nplt.plot(x,y_poly_pred,'red')\nplt.legend(['Raw Data',       \n            'y=%5.2f+%5.2f*x+%5.2f*x²+%5.2f*x³' % (LRmodel.intercept_, LRmodel.coef_[0][1],LRmodel.coef_[0][2],LRmodel.coef_[0][3])    \n            ], prop={'size': 8})\nplt.title('Polynomial Regression Model')\n\n\n# B2_Ch4_6_D.py\n# valuate model\nrmse = np.sqrt(mean_squared_error(y,y_poly_pred))\r2 = r2_score(y,y_poly_pred)\nprint('RMSE of this polynomial regression model: %.2f' % rmse)\nprint('R square of this polynomial regression model: %.2f' % r2)\n"
      },
      {
        "id": "ex8",
        "title": "4.8 參數顯著性與擬合檢定",
        "filename": "B2_Ch4_8.py",
        "code": "# B2_Ch4_8.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\nimport pandas as pd\nfrom scipy import stats \nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\outliersimpact.csv')\n\nX = df.x\ny = df.y\n\nplt.plot(X, y, 'bo')\n\nslope1, intercept1, r_value1, p_value1, std_err1 = stats.linregress(X, y)\nrline1 = intercept1 + slope1*X\n\nplt.plot(X, rline1,'r-', label='Fitting with outliers')\n\nplt.annotate('Fitting with outliers', xy=(0.6, intercept1 + slope1*0.6), xytext=(0.6, 1.2), \n              arrowprops=dict(arrowstyle=\"-|>\",\n                             connectionstyle=\"arc3\",\n                             mutation_scale=20,\n                             fc=\"w\"))\n\nplt.annotate('outliers', xy=(0.802171, 0.5), xytext=(0.75, 0.6))#, \n             # arrowprops=dict(arrowstyle=\"-|>\",\n             #                connectionstyle=\"arc3\",\n             #                mutation_scale=20,\n             #                fc=\"w\"))\nplt.annotate('', xy=(0.89286, 0.6), xytext=(0.75, 0.6))#, \n                          # arrowprops=dict(arrowstyle=\"-|>\",\n                          #               connectionstyle=\"arc3\",\n                          #               mutation_scale=20,\n                          #               fc=\"w\"))\n\n# eliminate two outliers\ndf_nooutliers = df[(df['y']!=0.5) & (df['y']!=0.6)]\n\nX = df_nooutliers.x\ny = df_nooutliers.y\n\nslope2, intercept2, r_value2, p_valu2e, std_err2 = stats.linregress(X, y)\nrline2 = intercept2 + slope2*X\nplt.plot(X, rline2,'r--', label='Fitting without outliers')\n\nplt.annotate('Fitting without outliers', xy=(0.7, intercept2 + slope2*0.7), xytext=(0.4, 2.2), \n              arrowprops=dict(arrowstyle=\"-|>\",\n                             connectionstyle=\"arc3\",\n                             mutation_scale=20,\n                             fc=\"w\"))\n\nplt.title('Impact on linear regression by outliers')\nplt.gca().set_yticks([0.5, 1.0, 1.5, 2.0, 2.5])\n\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().xaxis.set_ticks_position('bottom')\n"
      },
      {
        "id": "ex9",
        "title": "4.9 嶺回歸正則化參數分析",
        "filename": "B2_Ch4_9.py",
        "code": "# B2_Ch4_9.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\n# B2_Ch4_9_A.py\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\n\n\n# B2_Ch4_9_B.py\n# extract and plot raw data\ndata = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\RidgeRegrData.csv')\nplt.plot(data['x'], data['y'], 'o')\nplt.title('Raw Data')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().xaxis.set_ticks_position('bottom')\n\n\n# B2_Ch4_9_C.py\n# prepare data with powers up to 15\nfor i in range(2,16):  \n    colname = 'x_%d'%i      \n    data[colname] = data['x']**i\nprint(data.head())\n\n\n# B2_Ch4_9_D.py\n# create ridge regression fit and plot function\ndef ridge_regression_fit_plot(data, predictors, alpha, alpha_subplotpos):\n    # fit ridge regression model\n    ridgeregrmodel = Ridge(alpha=alpha, normalize=True)\n    ridgeregrmodel.fit(data[predictors], data['y'])\n    y_pred = ridgeregrmodel.predict(data[predictors])\n    \n    # plot for model with predefined alpha\n    if alpha in alpha_subplotpos:\n        plt.subplot(alpha_subplotpos[alpha])\n        plt.tight_layout()\n        plt.plot(data['x'], data['y'],'.')\n        plt.plot(data['x'], y_pred, 'g-')         \n        plt.title('Ridge Regression:  $\\\\alpha$=%.3g'%alpha)\n    \n    # return results\n    rss = sum((y_pred-data['y'])**2)\n    ret = [rss]\n    ret.extend([ridgeregrmodel.intercept_])\n    ret.extend(ridgeregrmodel.coef_)\n    return ret\n\n\n# B2_Ch4_9_E.py\n# initialize predictors to be set of 15 powers of x\npredictors=['x']\npredictors.extend(['x_%d'%i for i in range(2,16)])\n# set list of alpha values\nalpha_list = [1e-20, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1, 1, 2, 3, 5, 10, 20]\n# store coefficients\ncol = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\nind = ['alpha_%.2g' % alpha_list[i] for i in range(0,len(alpha_list))]\ncoef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n# alpha:subplot position\nalpha_subplotpos = {1e-20:241, 1e-10:242, 1e-3:243, 1e-2:244, 1e-1:245, 1:246, 5:247, 20:248}\nfor i in range(len(alpha_list)):\n    coef_matrix_ridge.iloc[i,] = ridge_regression_fit_plot(data, predictors, alpha_list[i], alpha_subplotpos)    \n\n\n\n# B2_Ch4_9_F.py\n# show parameter matrix\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_ridge\n\n# B2_Ch4_9_G.py\n# plot rss of models\nplt.plot(coef_matrix_ridge['rss'], 'o')\nplt.title('RSS Trend')\nplt.xlabel(r'$\\alpha$')\nplt.xticks(rotation=30)\nplt.ylabel('RSS')\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().xaxis.set_ticks_position('bottom')\n\n\n# B2_Ch4_9_H.py\ncoef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)\n"
      },
      {
        "id": "ex10",
        "title": "4.10 套索回歸與特徵選擇",
        "filename": "B2_Ch4_10.py",
        "code": "# B2_Ch4_10.py\n\n###############\n# Prepared by Ran An, Wei Lu, and Feng Zhang\n# Editor-in-chief: Weisheng Jiang, and Sheng Tu\n# Book 2  |  Financial Risk Management with Python\n# Published and copyrighted by Tsinghua University Press\n# Beijing, China, 2021\n###############\n\n# B2_Ch4_10_A.py\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\n\n# create lasso regression fit and plot function\ndef lasso_regression_fit_plot(data, predictors, alpha, alpha_subplotpos):\n    # fit lasso regression model\n    lassoregrmodel = Lasso(alpha=alpha, normalize=True, tol=0.1)\n    lassoregrmodel.fit(data[predictors], data['y'])\n    y_pred = lassoregrmodel.predict(data[predictors])\n    \n    # plot for model with predefined alpha\n    if alpha in alpha_subplotpos:\n        plt.subplot(alpha_subplotpos[alpha])\n        plt.plot(data['x'], data['y'],'.')\n        plt.plot(data['x'], y_pred, 'r')         \n        plt.title('$\\\\alpha$=%.3g'%alpha)\n    plt.yticks([-1.0, -0.5, 0, 0.5, 1.0])\n    \n    # return results\n    rss = sum((y_pred-data['y'])**2)\n    ret = [rss]\n    ret.extend([lassoregrmodel.intercept_])\n    ret.extend(lassoregrmodel.coef_)\n    return ret\n\n\n# B2_Ch4_10_B.py\n# extract raw data\ndata = pd.read_csv(r'C:\\Users\\anran\\Dropbox\\FRM Book\\Regression\\RidgeRegrData.csv')\n\n# prepare data with powers up to 15\nfor i in range(2,16):  \n    colname = 'x_%d'%i      \n    data[colname] = data['x']**i\n\n# initialize predictors to be set of 15 powers of x\npredictors=['x']\npredictors.extend(['x_%d'%i for i in range(2,16)])\n\n# set list of alpha values\nalpha_list = [1e-20, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1, 1, 2, 3, 5, 10, 20]\n\n# store coefficients\ncol = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\nind = ['alpha_%.2g'%alpha_list[i] for i in range(0,len(alpha_list))]\ncoef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n\n# alpha:subplot position\nalpha_subplotpos = {1e-20:231, 1e-10:232, 1e-5:233, 1e-3:234, 1e-2:235, 1e-1:236}\nfor i in range(len(alpha_list)):\n    coef_matrix_lasso.iloc[i,] = lasso_regression_fit_plot(data, predictors, alpha_list[i], alpha_subplotpos)\n\n\n# B2_Ch4_10_C.py\n# show parameter matrix\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_ridge\n\n\n# B2_Ch4_10_D.py\n# plot rss of models\nplt.plot(coef_matrix_lasso['rss'], 'o')\nplt.title('RSS Trend')\nplt.xlabel(r'$\\alpha$')\nplt.xticks(rotation=30)\nplt.ylabel('RSS')\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().xaxis.set_ticks_position('bottom')\n\n\n# B2_Ch4_10_E.py\ncoef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)\n"
      }
    ]
  }
}